---
title: "Data Science Essentials"
output: html_notebook
---

#Probability and statistics in R
Distributions with 
```{r}
library(foreach)
library(tidyverse)
n_simulations   <- 1000
nrows           <- 5000
sample_p        <- 0.7
dist_mean       <- 0
dist_sd         <- 1
pthreshold      <- 0.05 #for p-hacking!

# foreach is a tool for doing simulations
# the .combine 
simresult<-foreach(r=seq_len(n_simulations), .combine=c) %do% {
my_var <- rnorm(nrows, mean=dist_mean, sd=dist_sd) #Recieve values from the norm distribution

rows_in_training <- sample(1:nrows, nrows*sample_p) #Sample 0.7% of a sequence from 1 to 500

training <- my_var[rows_in_training]
testing <- my_var[-rows_in_training]

sim_test <- t.test(training, testing)
ifelse(sim_test$p.value > pthreshold, "the means are the same", "the means are not the same") 
}
fct_count(simresult) # fct_count builds a frequency table with n and prop as table headers that can be used to 

simresult %>%
  fct_count() %>%
  mutate(prop=scales::percent(n/sum(n)))
```

#Simulation and hypothesis testing in R

```{r}
# Now we are changing from simulations with fixed parameters to variables to be able to see what drives the differences in the test
library(foreach)
library(tidyverse)
library(parallel)
r_simulations   <- seq_len(1)
nrows           <- seq(5000, 200000, by=10000)
sample_p        <- c(0.7, 0.8, 0.9)
dist_mean       <- 0
dist_sd         <- 1:10
pthreshold      <- c(0.05, 0.1) #for p-hacking!

# how many simulations to do?
combos <-expand.grid(r=r_simulations
                ,n=nrows
                ,s=sample_p
                ,m=dist_mean
                ,sd=dist_sd
                ,p=pthreshold)
nrow(combos)

# foreach is a tool for doing simulations
# the .combine will output a vector (c)
                #r=r_simulations
                #,n=nrows
                #,s=sample_p
                #,m=dist_mean
                #,sd=dist_sd
                #,p=pthreshold
                #,.combine=rbind)
simresult<-foreach(t=1:nrow(combos), .combine = rbind) %do% {
  df <- combos[t, ]
  outcome_var <- rnorm(df$n, mean=df$m, sd=df$sd)
  use_for_training <- sample(1:df$n, df$n*df$s)
  training <-  outcome_var[use_for_training]
  testing <-  outcome_var[-use_for_training]
  sim_test <- t.test(training, testing)
#my_var <- rnorm(n, mean=m, sd=sd) #Recieve values from the norm distribution
#rows_in_training <- sample(1:n, n*s) #Sample 0.7% of a sequence from 1 to 500
df$result <-ifelse(sim_test$p.value > df$p, "the means are the same", "the means are not the same")
 df
}


#fct_count(simresult) # fct_count builds a frequency table with n and prop as table headers that can be used to 

simresult %>%
  count(result) %>%
  mutate(prop=scales::percent(nn/sum(nn)))

```
```{r}
# Now we are changing from simulations with fixed parameters to variables to be able to see what drives the differences in the test, and the probability to get a bad sample. 
library(foreach)
library(tidyverse)
library(parallel) # detect cores on windows machines and sockets on linux machines. use all four cores 
library(doParallel) # to be able to use %dopar%
my_machine <- parallel::makeCluster(detectCores())
registerDoParallel(my_machine) # when we use parallel code this is registred

r_simulations   <- seq_len(100)
nrows           <- seq(5000, 200000, by=10000)
sample_p        <- c(0.7, 0.8, 0.9)
dist_mean       <- 0
dist_sd         <- 1:10
pthreshold      <- c(0.05, 0.1) #for p-hacking!

# how many simulations to do? use expand grid to get all combinations of factors
combos <-expand.grid(r=r_simulations
                ,n=nrows
                ,s=sample_p
                ,m=dist_mean
                ,sd=dist_sd
                ,p=pthreshold)
nrow(combos)

# foreach is a tool for doing simulations
# the .combine will output a vector (c)

simresult<-foreach(t=1:nrow(combos), .combine = rbind) %dopar% {
  df <- combos[t, ]
  outcome_var <- rnorm(df$n, mean=df$m, sd=df$sd)
  use_for_training <- sample(1:df$n, df$n*df$s)
  training <-  outcome_var[use_for_training]
  testing <-  outcome_var[-use_for_training]
  sim_test <- t.test(training, testing)
  df$result <-ifelse(sim_test$p.value > df$p, "the means are the same", "the means are not the same")
 
  df
}


#fct_count(simresult) # fct_count builds a frequency table with n and prop as table headers that can be used to 

simresult %>%
  count(result) %>%
  mutate(prop=scales::percent(nn/sum(nn)))

library(ggplot2)
simresult %>%
  mutate(sim=row_number()) %>%
  gather(lever, value, -r, -sim, -result) %>% #use unpivot by gather function
 # group_by(lever, result) %>%
  count(lever, value, result) %>%
  ggplot(aes(x=value, y=n, group=result, colour=result)) + 
  geom_col() +
  facet_wrap(~lever, scales="free")

library(ggplot2)
simresult %>%
  mutate(sim=row_number()) %>%
  gather(lever, value, -r, -sim, -result) %>% #use unpivot by gather function
 # group_by(lever, result) %>%
  count(lever, value, result) %>%
  filter(result!="the means are the same") %>%
  filter(lever!="m") %>%
  #mutate(prop=scales) %>%
  ggplot(aes(x=value, y=n, group=result)) +
  geom_line() +
  #scale_x_discrete() +
  facet_wrap(~lever, scales="free") +
  theme_minimal() +
  geom_smooth()
  
```
#Prediction
Step 1: The first We try to use the DataExplorer and the AlzheimerDisease dataset in the AppliedPredictiveModeling package.
```{r}
library(tidyverse)
library(AppliedPredictiveModeling)
library(skimr)

data(AlzheimerDisease) #Load data and combine the two datasets into one, and name it alzheimers
predictors %>%
  cbind(diagnosis) -> alzheimers

DataExplorer::create_report(alzheimers, "alzheimers.html") # the full report of the dataset as a html site

alzheimers %>%
  ggplot(aes(x=Thyroid_Stimulating_Hormone)) + geom_histogram() # one variable that we inspect

alzheimers %>%
  filter(Thyroid_Stimulating_Hormone == min(Thyroid_Stimulating_Hormone)) %>% 
  skimr::skim()# in the minimum value of this variable, do they have other values that looks weird? the skim looks at the distribution of the data we have chosen, in this case the minimum value. 

```
# Cleaning data and manupulate data
By looking at the plots in our data exploration, we will do some manipulation and cleaning. 

dplyr verb conventions
- verb itself e.g. 'select()' allows you to manually write out selections
- verb suffixed '_if' e.g. 'select_if()'you can write predicates/conditions based on the column contents
- verb suffixed '_at'e.g. 'select_at()' you can write predicates/conditions based on the column name
- verb suffixed '_all' e.g. 'select_all()'you apply a function to every column

```{r}
alzheimers %>%
  mutate(malenew=factor(male)) %>% # first check if it works before we change the male variable to factor forever
  count(malenew,male) 

alzheimers %>%
  mutate(male=factor(male),
  Genotype = fct_lump(Genotype,n=3)) %>%# now we can combine three of the Genotypes into one by using lump. the n  is the one we don't want to lump, aka the ones not in the first 3. 
  count(Genotype)

alzheimers %>%
  mutate(male=factor(male),
  Genotype = fct_infreq(fct_lump(Genotype,n=3))) -> alzheimers

# now we can combine three of the Genotypes into one by using lump. the n  is the one we don't want to lump, aka the ones not in the first 3. by using the fct_infreq, we use the default variable value as the most common one, otherwise the default will be the one first in the alphabet. 
  count(Genotype)
  
  
```
#Sampling
Now our data is in better order, so we go into sampling. We want to scale/normalize our data to be able to compare them and make sure the future models on operational data will work as intended. To avoid overfitting we must first so some

```{r}
library(rsample) # makes us do samples of our data, by using training() and testing()
library(recipes)

alzheimers %>%
  initial_split(prop = .9) ->
  alz_split

alz_split %>%
  training() ->
  alz_train

alz_split %>%
  testing() -> 
  alz_test
  
# We want to scale/normalize our data to be able to compare them and make sure the future models on operational data will work as intended.

```

#Data manipulation
By using recipe package, we make sure the preprocessing manipulation steps works for future data. Look into the step_ functions in the recipe package to get help in preprocessing. There are steps for handling missing values, removing columns and much more. 
```{r}
#construct a recipe by adding ingredients in certain order before we go into baking

#Scaling/basics process
alz_train %>%
  recipe(diagnosis ~ ., .) %>% # don't touch the variable diagnosis, since it's the one we want to predict. this will be saved so that no other recipes over-write it. 
  step_center(all_numeric()) %>% # take all num predicting variables into the preprocessing
  step_scale(all_numeric()) %>%
  prep(training=alz_train) ->
  alz_preprocess # save as preprocessing step that we then can use for baking purposes
  
# Feature reduction
alz_preprocess %>%
  step_corr(all_numeric()) %>% #remove correlated predictor variables, and will keep the one that are most independent in comparision to all the other variables in the model. 
  step_nzv(all_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_upsample(diagnosis) %>% 
  step_pca(all_numeric()) %>%
  prep(training=alz_train, retain=TRUE) ->
  alz_preprocess

alz_preprocess %>% #use all the preprocessing steps on the train data (we don't need this if using juice instead)
  bake(alz_train) ->
  alz_train_p

alz_preprocess %>% #Squeezing out some extra rows from our train data AND apply the preprocessing steps
  juice(all_outcomes(),all_predictors())->
  alz_train_p

alz_preprocess %>% #Only use the preprocess step for the test data
  bake(alz_test) ->
  alz_test_p

alz_preprocess

```
#Modelling
Begin classification model

```{r}
rm("diagnosis")
alz_train_p %>%
  glm(diagnosis~., data=., family="binomial") -> #tell the glm that we are using all variable by using ~. remember to tell glm what distribution family we should run by
  alz_glm_full
```

#Evaluate and score model
Show the fit of the built model and the evaluating values. 
```{r}
rm(View)
library(broom) # Convert Statistical Analysis Objects into Tidy Data Frames
library(yardstick) # A tidy tool package for quantifying how well model fits to a data set such as confusion matrces, class probability curve summaries, and regression metrics (e.g., RMSE). 
library(modelr) # Functions for modelling that help you seamlessly integrate modelling into a pipeline of data manipulation and visualisation.

alz_glm_full %>%
  broom::glance() #show us the measure of fit as a table 

alz_glm_full %>%
  broom::tidy() #show us the output of the model (estimate, std.error, statistic, p-values)

alz_glm_full %>%
  broom::augment()%>%
  View()

alz_test_p %>% 
  modelr::add_predictions(alz_glm_full) %>% 
  mutate(class=factor(ifelse(pred>=0,"Control", "Impaired"),levels=c("Impaired","Control")))->
  alz_test_scored 

alz_test_scored %>% View()

alz_test_scored %>% 
  yardstick::accuracy(diagnosis,class)

#Make decision tree on how well our model performed, actually build multiple trees by CART, SVM etc. 
library(FFTrees)
alz_glm_full %>% 
  broom::augment() %>% 
  select(diagnosis:.fitted) %>% 
  mutate(class=factor(ifelse(.fitted >=0, "Control", "Impaired"),
                      levels = c("Impaired","Control"))) %>% 
  mutate(correct=diagnosis==class) %>% 
  select(-.fitted, -class) %>% 
  FFTrees(correct~.,data=.) %>% 
  plot()
```

